\documentclass[twocolumn]{article}

\usepackage{setspace} % for \onehalfspacing and \singlespacing macros
\onehalfspacing

\usepackage{amsmath}
\usepackage{graphics}

\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{svg}

\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\par\singlespacing\small}

\title{Notes on Math and Architectures of Deep Learning}
\author{Benjamin Santana}
\date{April 2023}

\begin{document}
   \maketitle

\section{Introduction}

Before jumping into deep learning, let us talk about machine learning a bit.
What is machine learning? Living creatures are taking decisions at every moment.
Say for a example a dog sees an object and has to take a decision, does it get
away from it, does it runs to towards it, or does it simply ignore it.

This will depend on certain factors about the object, is it sharp? does it
looks cozy? Believe it or not the dog has a machine inside its head, taking
this decisions. Its brain.

Machine learning is trying to emulate this machine, taking different parameters
and output a decision.

Computers already do this, but the new thing is that now we do not give the
computer step by step instructions on how to take the decision. Instead we
develop a mathematical model for the problem.

Let us continue with the dog example I was taking about (which I completely
stole from the book).

We have two \emph{features}, hardness and sharpness.

We could define a \emph{parameterized function} with a \emph{weighted sum of
inputs}:

\noindent
\begin{multline}
    y(hardness, sharpness) = \\ w_0 \cdot hardness + w_1 \cdot sharpness + b
\end{multline}

The weights \emph{w0} , \emph{w1} and the bias \emph{b} are the parameters of
the function. The output \emph{y} can be interpreted as a threat score.

The weights are not know at first, we need to estimate them, this is what we
call \emph{model training}.

The steps to solve any machine learning problem are the following:
\begin{enumerate}
    \item We first design a parameterized model function (e.g. weighted sum)
        with unknown parameters (weights). This makes the \emph{model
        architecture}.
    \item Estimate the weights via model training.
    \item Now we have a complete model, we can send inputs not seen before and
        it should work, this is called \emph{inference}.
\end{enumerate}

In supervised learning, us humans will create the training data, this means
creating example inputs, each corresponding with the desired output.

The training the model part is iterating through different weight values and
checking with the training data to see if we are close to the desire output.

We optimize the way we look for those weights, we will see that in detail in
the future.

At first, our model will be dumb, it will not be able to perform well, like a
baby. But once we have run some iterations and we correct it, it will start
performing better.

\section{Simple Model}

The book used a simple example on how to apply machine learning to the dog
problem we stated above, in the book it was a cat but I am more of a dog person
so...

We have a hypothetical dog which only needs to take one decision in life, run
away from an object, ignore it, or approach it. It takes this decision based
only on two quantitative inputs.

\subsection{Input Features}

{$ \displaystyle x_0 = hardness,$}
{$ x_1 = sharpness $}

we can normalize this input

\begin{equation}
v_{norm} = \dfrac{(v - v_{min})}{(v_{max} - v_{min})}
\end{equation}

where {$v_{min}$} is the min value possible and {$v_{max}$} is the max value
possible.

This will generate values between $1$ and $0$, thus we can define the input as

{$ v \in [v_{min}, v_{max}] \rightarrow v_{norm} \in [0, 1] $}

A single input becomes

\begin{equation}
    \vec{x} = \begin{bmatrix}
                x_0 \\
                x_1 \\
              \end{bmatrix},
              \in [0, 1]^2
\end{equation}

\subsection{Output Decisions}

We will have our model estimate a threat score ($y$) and based on a threshold
($\delta$) we will assign a class to it. If $y > \delta$ high threat, run away.
If $y$ is around 0, do nothing. If $y < \delta$ negative threat,
approach.

\subsection{Model Estimation}

We need to estimate the function which transforms the input vector to the
output. This consists on two parts

\begin{itemize}
  \item model architecture selection
  \item model training
\end{itemize}

\subsubsection{Model Architecture Selection}

Our model has three parameters $w_0, w_1, b$ they can be represented as

\begin{equation}
    \vec{w} = \begin{bmatrix}
                w_0 \\
                w_1 \\
              \end{bmatrix},
\end{equation}

and $b$ as a scalar. All three being real numbers. We will define a super
simple function such as

\noindent
\begin{equation}
    y(x_0, x_1) = w_0  x_0 + w_1  x_1 + b = \vec{w}^T\vec{x} + b
\end{equation}
\noindent

Where $b$ is the bias.

\begin{equation}
    y(\vec{x}) = \vec{w}^T\vec{x} + b
\end{equation}


\subsubsection{model training}

We defined $y(\vec{x})$, now we have to find the best weights for the function.
We will choose the parameters so that the outputs on the training data inputs,
match the corresponding outputs as close as possible.

How do we calculate the error?

On the $i^{th}$ training data pair $(\vec{x}(i), y_{gt}(i))$ ($gt$ ground
truth) we can say $y_{predicted}(i) = \vec{w}^{T}\vec{x}(i) + b$, then we
compare
$y_{predicted}(i)$ with $y_{gt}$

\begin{equation}
    e(i)^2 = (y_{predicted}(i) - y_{gt})^2
\end{equation}

It is squared to take into account negative values.

This way we calculate the error of one iteration, the overall error (aka loss)
is the sum of all $e(i)^2$

\begin{equation}
    E^2 = \sum_{i=0}^{i=N} e(i)^2
\end{equation}

our goal is to find the $\vec{w}$ that minimizes $E^2$.

The ideal goal would be to actually get $E^2$ to 0, but in practice, one often
stops when the results are accurate enough to solve the problem we are
presented.

\subsection{Geometrical View of Machine Learning}

In the model specified above, each input is a vector with 2 values $x_0$ and
$x_1$ (hardness and sharpness). If we map this into a 2D space, each input is a
point on the plane. Usually it would have more dimensions, but this being a
super simple problem only have two. This space is called \emph{feature space}.

The model will work as a translator, that takes points in the feature space and
map them to another space with less dimensions called, the \emph{output space}
(they really break their heads thinking for a name for this last one).

It is supposed to be easier for the computer to take decisions in the
\emph{output space}

\subsection{Regression vs Classification in ML}

We have two types of machine learning models: \emph{regressors} and
\emph{classifiers}.

A regressor attempts to provide us with a value based on a specific input,
while classifiers aim to assign a class to which the input belongs.

It is possible to create a regressor and then, based on a threshold, select a
class. Alternatively, a model can directly output one of these predefined
classes.

\subsection{Linear vs Nonlinear Models}

\begin{figure}[htbp]
  \centering
  \includesvg[width=1\columnwidth]{images/classify-nolinear.svg}
  \caption{Feature map where we cannot use a linear function to classify the
    inputs}
  \label{fig:nolinear}
\end{figure}

Usually when we try and classify points, they are not in a clear straight
line. Meaning that Function~\ref{eq:yxw} not going to cut it. This is also
shown in Figure~\ref{fig:nolinear}.

\begin{equation}
    y(\vec{x}) = \vec{w}^T\vec{x} + b
    \label{eq:yxw}
\end{equation}

We will need a non-linear function to classify these points. A popular
function in machine learning is the Sigmoid function, (named because of its S
shape). Shown in Equation~\ref{eq:sigmoid}

\begin{equation}
    \sigma(x) = \frac{1}{1 + e^{-x}}
    \label{eq:sigmoid}
\end{equation}

Thus we can use the model architecture shown in
Equation~\ref{eq:sigmoid-model-arch}. It is the building blocks of a neural
network.

\begin{equation}
    y = \sigma(\vec{w}^T\vec{x} + b)
    \label{eq:sigmoid-model-arch}
\end{equation}


\end{document}
