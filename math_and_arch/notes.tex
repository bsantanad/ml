\documentclass[twocolumn]{article}

\usepackage{setspace} % for \onehalfspacing and \singlespacing macros
\onehalfspacing

\usepackage{amsmath}
\usepackage{graphics}

\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\par\singlespacing\small}

\title{Notes on Math and Architectures of Deep Learning}
\author{Benjamin Santana}
\date{April 2023}

\begin{document}
   \maketitle

\section{introduction}

Before jumping into deep learning, let us talk about machine learning a bit.
What is machine learning? Living creatures are taking decisions at every moment.
Say for a example a dog sees an object and has to take a decision, does it get
away from it, does it runs to towards it, or does it simply ignore it.

This will depend on certain factors about the object, is it sharp? does it
looks cozy? Believe it or not the dog has a machine inside its head, taking
this decisions. Its brain.

Machine learning is trying to emulate this machine, taking different parameters
and output a decision.

Computers already do this, but the new thing is that now we do not give the
computer step by step instructions on how to take the decision. Instead we
develop a mathematical model for the problem.

Let us continue with the dog example I was taking about (which I completely
stole from the book).

We have two \emph{features}, hardness and sharpness.

We could define a \emph{parameterized function} with a \emph{weighted sum of
inputs}:

\noindent
\begin{multline}
    y(hardness, sharpness) = \\ w_0 \cdot hardness + w_1 \cdot sharpness + b
\end{multline}

The weights \emph{w0} , \emph{w1} and the bias \emph{b} are the parameters of
the function. The output \emph{y} can be interpreted as a threat score.

The weights are not know at first, we need to estimate them, this is what we
call \emph{model training}.

The steps to solve any machine learning problem are the following:
\begin{enumerate}
    \item We first design a parameterized model function (e.g. weighted sum)
        with unknown parameters (weights). This makes the \emph{model
        architecture}.
    \item Estimate the weights via model training.
    \item Now we have a complete model, we can send inputs not seen before and
        it should work, this is called \emph{inference}.
\end{enumerate}

In supervised learning, us humans will create the training data, this means
creating example inputs, each corresponding with the desired ouput.

The training the model part is iterating through different weight values and
checking with the training data to see if we are close to the desire output.

We optimize the way we look for those weights, we will see that in detail in
the future. 

At first, our model will be dumb, it will not be able to perform well, like a
baby. But once we have run some iterations and we correct it, it will start
performing better.

\section{simple model}

The book used a simple example on how to apply machine learning to the dog
problem we stated above, in the book it was a cat but I am more of a dog person
so...

We have a hypothetical dog which only needs to take one decision in life, run
away from an object, ignore it, or approach it. It takes this decision based
only on two quantitative inputs.

\subsection{input features}

{$ \displaystyle x_0 = hardness,$}
{$ x_1 = sharpness $}

we can normalize this input

\begin{equation}
v_{norm} = \dfrac{(v - v_{min})}{(v_{max} - v_{min})}
\end{equation}

where {$v_{min}$} is the min value possible and {$v_{max}$} is the max value
possible.

This will generate values between $1$ and $0$, thus we can define the input as 

{$ v \in [v_{min}, v_{max}] \rightarrow v_{norm} \in [0, 1] $} 

A single input becomes

\begin{equation}
    \vec{x} = \begin{bmatrix}
                x_0 \\
                x_1 \\
              \end{bmatrix}, 
              \in [0, 1]^2
\end{equation}

\subsection{output decisions}

We will have our model estimate a threat score ($y$) and based on a threshold
($\delta$) we will assing a class to it. If $y > \delta$ high threat, run away.
If $y$  is around 0, do nothing. If $y < \delta$ negative threat,
approach.

\subsection{model estimation}

We need to estimate the function which transforms the input vetor to the
output. This consists on two parts

\begin{itemize}
  \item model architecture selection
  \item model training
\end{itemize}

\subsubsection{model architecture selection}

Our model has three parameters $w_0, w_1, b$ they can be represented as 

\begin{equation}
    \vec{w} = \begin{bmatrix}
                w_0 \\
                w_1 \\
              \end{bmatrix}, 
\end{equation}

and $b$ as a scalar. All three being real numbers. We will define a super
simple function such as 

\noindent
\begin{equation}
    y(x_0, x_1) = w_0  x_0 + w_1  x_1 + b = \vec{w}^T\vec{x} + b
\end{equation}

Where $b$ is the bias.

\subsubsection{model training}

We defined $y(\vec{x})$, now we have to find the best weights for the function.
We will choose the parameters so that the outputs on the training data inputs,
match the corresponding outputs as close as possible.

How do we calculate the error? 

On the $i^{th}$ training data pair $(\vec{x}(i), y_{gt}(i))$ ($gt$ ground
truth) we can say $y_{predicted}(i) = \vec{w}^{T}\vec{x}(i) + b$, then we
compare
$y_{predicted}(i)$ with $y_{gt}$

\begin{equation}
    e(i)^2 = (y_{predicted}(i) - y_{gt})^2
\end{equation}

It is squared to take into account negative values.

This way we calculate the error of one iteration, the overal error (aka loss)
is the sum of all $e(i)^2$

\begin{equation}
    E^2 = \sum_{i=0}^{i=N} e(i)^2
\end{equation}

our goal is to find the $\vec{w}$ that minimizes $E^2$.

But in practice, one often stops when the results are accurate enough to solve
the problem we are presented.


\end{document}
